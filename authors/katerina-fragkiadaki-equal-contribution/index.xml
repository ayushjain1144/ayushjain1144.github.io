<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Katerina Fragkiadaki (* Equal Contribution) | Ayush Jain</title>
    <link>http://localhost:1313/authors/katerina-fragkiadaki-equal-contribution/</link>
      <atom:link href="http://localhost:1313/authors/katerina-fragkiadaki-equal-contribution/index.xml" rel="self" type="application/rss+xml" />
    <description>Katerina Fragkiadaki (* Equal Contribution)</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 09 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/img/icon-192.png</url>
      <title>Katerina Fragkiadaki (* Equal Contribution)</title>
      <link>http://localhost:1313/authors/katerina-fragkiadaki-equal-contribution/</link>
    </image>
    
    <item>
      <title>Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds</title>
      <link>http://localhost:1313/publication/beauty-detr/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/beauty-detr/</guid>
      <description>&lt;p&gt;Most models tasked to ground referential utterances in 2D and 3D scenes learn to select the referred object from a pool of object proposals provided by a pre-trained detector. This is limiting because an utterance may refer to visual entities at various levels of granularity, such as the chair, the leg of the chair, or the tip of the front leg of the chair, which may be missed by the detector. We propose a language grounding model that attends on the referential utterance and on the object proposal pool computed from a pre-trained detector to decode referenced objects with a detection head, without selecting them from the pool. In this way, it is helped by powerful pre-trained object detectors without being restricted by their misses. We call our model Bottom Up Top Down DEtection TRansformers (BUTD-DETR) because it uses both language guidance (top down) and objectness guidance (bottom-up) to ground referential utterances in images and point clouds. Moreover, BUTD-DETR casts object detection as referential grounding and uses object labels as language prompts to be grounded in the visual scene, augmenting supervision for the referential grounding task in this way. The proposed model sets a new state-of-the-art across popular 3D language grounding benchmarks with significant performance gains over previous 3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When applied in 2D images, it performs on par with the previous state of the art. We ablate the design choices of our model and quantify their contribution to performance. Our code and checkpoints can be found at the project website: &lt;a href=&#34;https://butd-detr.github.io/&#34;&gt;https://butd-detr.github.io/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Move to See Better: Towards Self-Improving Embodied Object Detection</title>
      <link>http://localhost:1313/publication/seeingbymoving/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/seeingbymoving/</guid>
      <description>&lt;p&gt;Humans learn to better understand the world by moving around their environment to get more informative viewpoints of the scene. Most methods for 2D visual recognition tasks such as object detection and segmentation treat images of the same scene as individual samples and do not exploit object permanence in multiple views. Generalization to novel scenes and views thus requires additional training with lots of human annotations. In this paper, we propose a self-supervised framework to improve an object detector in unseen scenarios by moving an agent around in a 3D environment and aggregating multi-view RGB-D information. We unproject confident 2D object detections from the pre-trained detector and perform unsupervised 3D segmentation on the point cloud. The segmented 3D objects are then re-projected to all other views to obtain pseudo-labels for fine-tuning. Experiments on both indoor and outdoor datasets show that (1) our framework performs high quality 3D segmentation from raw RGB-D data and a pre-trained 2D detector; (2) fine-tuning with self supervision improves the 2D detector significantly where an unseen RGB image is given as input at test time; (3) training a 3D detector with self supervision outperforms a comparable self-supervised method by a large margin.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
