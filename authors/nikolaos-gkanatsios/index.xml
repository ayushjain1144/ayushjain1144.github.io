<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nikolaos Gkanatsios* | Ayush Jain</title>
    <link>https://ayushjain1144.github.io/authors/nikolaos-gkanatsios/</link>
      <atom:link href="https://ayushjain1144.github.io/authors/nikolaos-gkanatsios/index.xml" rel="self" type="application/rss+xml" />
    <description>Nikolaos Gkanatsios*</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 09 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ayushjain1144.github.io/img/icon-192.png</url>
      <title>Nikolaos Gkanatsios*</title>
      <link>https://ayushjain1144.github.io/authors/nikolaos-gkanatsios/</link>
    </image>
    
    <item>
      <title>Language Modulated Detection and Detection Modulated Language Grounding in 2D and 3D Scenes</title>
      <link>https://ayushjain1144.github.io/publication/beauty-detr/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://ayushjain1144.github.io/publication/beauty-detr/</guid>
      <description>&lt;p&gt;Existing language grounding models often use object proposal bottlenecks: a pre-trained detector proposes objects in the scene and the model learns to select the answer from these box proposals, without attending to the original image or 3D point cloud. Object detectors are typically trained on a fixed vocabulary of objects and attributes that is often too restrictive for open-domain language grounding, where an utterance may refer to visual entities at various levels of abstraction, such as a chair, the leg of a chair, or the tip of the front leg of a chair.&lt;br&gt;
We propose a model for grounding language in 3D scenes that bypasses box proposal bottlenecks with three main innovations: i) Iterative attention across the language stream, the point cloud feature stream and 3D box proposals. ii) Transformer decoders with non-parametric entity queries that decode 3D boxes for object and part referentials. iii) Joint supervision from 3D object annotations and language grounding annotations, by treating object detection as grounding of referential utterances comprised of a list of candidate category labels. These innovations result in significant quantitative gains (up to +9% absolute improvement on the SR3D benchmark) over previous approaches on popular 3D language grounding benchmarks. We ablate each of our innovations to show its contribution to the performance of the model. When applied on language grounding on 2D images with minor changes, it performs  on par with the state-of-the-art while converges  in  half of the GPU time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping Instructions to Goals for Robot Manipulation via  Graph-Structured Energy-Based Concept Models</title>
      <link>https://ayushjain1144.github.io/publication/ns_transporter/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://ayushjain1144.github.io/publication/ns_transporter/</guid>
      <description>&lt;p&gt;We propose a framework for robot instruction that maps language instructions to goal scene configurations of the relevant object and part entities, and their locations in the scene. The imagined goals modulate the pick and placement locations of a robotic gripper within a transporter network\cite{transporter} that re-arranges the objects in the scene. Our parser maps language commands to compositions of language-conditioned energy-based models\cite{mordatch2018concept} that generate the scene in a modular and compositional way.  We show the proposed model can follow instructions zero shot, without ever having seen an instruction (e.g. &amp;ldquo;make a circle&amp;rdquo;) paired with a set of corresponding actions, by sharing information regarding how objects are picked and placed across multiple different tasks. We show extensive results in simulation and the real world where our agent infers object re-arrangements from language with very little data shared across multiple instruction tasks and outperforms by a margin in out-of-domain generalization existing state-of-the-art language-conditioned transporter works that map language to actions directly. We consistent show  dramatic improvements  in long horizon manipulation tasks, such as making lines and circles, over existing state of the art image to action mapping methods, as well as in generalization to unseen visual conditions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
