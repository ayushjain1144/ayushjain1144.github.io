[{"authors":["**Ayush Jain***","Nikolaos Gkanatsios*","Ishita Mediratta","Katerina Fragkiadaki"],"categories":null,"content":"To localize an object referent, humans attend to different locations in the scene and visual cues depending on the utterance. Existing language and vision systems often model such task-driven attention using object proposal bottlenecks: a pre-trained detector proposes objects in the scene, and the model is trained to selectively process those proposals and then predict the answer without attending to the original image. Object detectors are typically trained on a fixed vocabulary of objects and attributes that is often too restrictive for open-domain language grounding, where the language utterance may refer to visual entities in various levels of abstraction, such as a cat, the leg of a cat, or the stain on the front leg of the chair. This paper proposes a model that reconciles language grounding and object detection with two main contributions: i) Architectures that exhibit iterative attention across the language stream, the pixel stream, and object detection proposals. In this way, the model learns to condition on easy-to-detect objects (e.g., table”) and language hints (e.g. on the table”) to detect harder objects (e.g., ``mugs”) mentioned in the utterance. ii) Optimization objectives that treat object detection as language grounding of a large predefined set of object categories. In this way, cheap object annotations are used to supervise our model, which results in performance improvements over models that are not co-trained across both referential grounding and object detection. Our model has a much lighter computational footprint, achieves faster convergence and has shown on par or higher performance compared to both detection-bottlenecked and non-detection bottlenecked language-vision models on both 2D and 3D language grounding benchmarks.\n","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604880000,"objectID":"353d41e9211e4a4443f0764ecbc735f0","permalink":"https://ayushjain1144.github.io/publication/beauty-detr/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/publication/beauty-detr/","section":"publication","summary":"To localize an object referent, humans attend to different locations in the scene and visual cues depending on the utterance. Existing language and vision systems often model such task-driven attention using object proposal bottlenecks: a pre-trained detector proposes objects in the scene, and the model is trained to selectively process those proposals and then predict the answer without attending to the original image. Object detectors are typically trained on a fixed vocabulary of objects and attributes that is often too restrictive for open-domain language grounding, where the language utterance may refer to visual entities in various levels of abstraction, such as a cat, the leg of a cat, or the stain on the front leg of the chair.","tags":null,"title":"Language Modulated Detection and Detection Modulated Language Grounding in 2D and 3D Scenes","type":"publication"},{"authors":["**Ayush Jain***","Gabriel Sarch*","Zhaoyuan Fang*","Adam Harley","Katerina Fragkiadaki"],"categories":null,"content":"Humans learn to better understand the world by moving around their environment to get more informative viewpoints of the scene. Most methods for 2D visual recognition tasks such as object detection and segmentation treat images of the same scene as individual samples and do not exploit object permanence in multiple views. Generalization to novel scenes and views thus requires additional training with lots of human annotations. In this paper, we propose a self-supervised framework to improve an object detector in unseen scenarios by moving an agent around in a 3D environment and aggregating multi-view RGB-D information. We unproject confident 2D object detections from the pre-trained detector and perform unsupervised 3D segmentation on the point cloud. The segmented 3D objects are then re-projected to all other views to obtain pseudo-labels for fine-tuning. Experiments on both indoor and outdoor datasets show that (1) our framework performs high quality 3D segmentation from raw RGB-D data and a pre-trained 2D detector; (2) fine-tuning with self supervision improves the 2D detector significantly where an unseen RGB image is given as input at test time; (3) training a 3D detector with self supervision outperforms a comparable self-supervised method by a large margin.\n","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604880000,"objectID":"222ba68cd34653b4e16537df57d98744","permalink":"https://ayushjain1144.github.io/publication/seeingbymoving/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/publication/seeingbymoving/","section":"publication","summary":"Humans learn to better understand the world by moving around their environment to get more informative viewpoints of the scene. Most methods for 2D visual recognition tasks such as object detection and segmentation treat images of the same scene as individual samples and do not exploit object permanence in multiple views. Generalization to novel scenes and views thus requires additional training with lots of human annotations. In this paper, we propose a self-supervised framework to improve an object detector in unseen scenarios by moving an agent around in a 3D environment and aggregating multi-view RGB-D information.","tags":null,"title":"Move to See Better: Towards Self-Improving Embodied Object Detection","type":"publication"},{"authors":["**Ayush Jain***","Rohit Ramaprasad*","Pratik Narang","Murari Mandal","Vinay Chamola","F. Richard Yu","Mohsen Guizani"],"categories":null,"content":"Unmanned Aerial Vehicles (UAVs) are emerging as a powerful tool for various industrial and smart city applications. The UAVs coupled with various sensors can perform many cognitive tasks such as object detection, surveillance, traffic management, and urban planning. These tasks often rely on computationally expensive deep learning approaches. Execution of the compute intensive algorithms are usually not feasible with the embedded processors on a power-constrained UAV. Therefore, the Edge-AI has emerged as a popular alternative in such scenarios by offloading the heavy-lifting tasks to the Edge devices. This work proposes a deep learning approach for detection of objects in aerial scenes captured by UAVs. In our setup, the power-constrained drone is used only for data collection, while the computationally intensive tasks are offloaded to a GPU edge server. Our work first categorize the current methods for aerial object detection using deep learning techniques and discusses how the task is different from general object detection scenarios. We delineate the specific challenges involved and experimentally demonstrate the key design decisions which significantly affect the accuracy and robustness of model. We further propose an optimized architecture which utilizes these optimal design choices along with the recent ResNeSt backbone in order to achieve superior performance in aerial object detection. Lastly, we propose several research directions to inspire further advancement in aerial object detection.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"b21fa14ddc93d5eab2848d121bf265af","permalink":"https://ayushjain1144.github.io/publication/aerial_object_detection/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/aerial_object_detection/","section":"publication","summary":"Unmanned Aerial Vehicles (UAVs) are emerging as a powerful tool for various industrial and smart city applications. The UAVs coupled with various sensors can perform many cognitive tasks such as object detection, surveillance, traffic management, and urban planning. These tasks often rely on computationally expensive deep learning approaches. Execution of the compute intensive algorithms are usually not feasible with the embedded processors on a power-constrained UAV. Therefore, the Edge-AI has emerged as a popular alternative in such scenarios by offloading the heavy-lifting tasks to the Edge devices.","tags":null,"title":"AI-enabled Object Detection in Unmanned Aerial Vehicles for Edge Computing Applications","type":"publication"},{"authors":["Dawei Du","Longyin Wen","Pengfei Zhu","Heng Fan","Qinghua Hu","Haibin Ling","Mubarak Shah","Junwen Pan","**Ayush Jain**","Pratik Narang","et al."],"categories":null,"content":"The Vision Meets Drone Object Detection in Image Challenge (VisDrone-DET 2020) is the third annual object detector benchmarking activity. Compared with the previous VisDrone-DET 2018 and VisDrone-DET 2019 challenges, many submitted object detectors exceed the recent state-of-the-art detectors. Based on the selected 29 robust detection methods, we discuss the experimental results comprehensively, which shows the effectiveness of ensemble learning and data augmentation in drone captured object detection. The full challenge results are publicly available at the website http://aiskyeye.com/leaderboard/.\n","date":1595980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595980800,"objectID":"d184cec039e052a88c8cc200c0da9c82","permalink":"https://ayushjain1144.github.io/publication/visdron/","publishdate":"2020-07-29T00:00:00Z","relpermalink":"/publication/visdron/","section":"publication","summary":"The Vision Meets Drone Object Detection in Image Challenge (VisDrone-DET 2020) is the third annual object detector benchmarking activity. Compared with the previous VisDrone-DET 2018 and VisDrone-DET 2019 challenges, many submitted object detectors exceed the recent state-of-the-art detectors. Based on the selected 29 robust detection methods, we discuss the experimental results comprehensively, which shows the effectiveness of ensemble learning and data augmentation in drone captured object detection. The full challenge results are publicly available at the website http://aiskyeye.","tags":null,"title":"VisDrone-DET2020: The Vision Meets Drone Object Detection in Image Challenge Results","type":"publication"},{"authors":null,"categories":null,"content":"","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"1a032ba59ab990085b0223efdbe424ae","permalink":"https://ayushjain1144.github.io/project/grnn/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/project/grnn/","section":"project","summary":"Reviewed, compared and analysed three recent papers proposed in generalising to novel views through the use of view-prediction self-supervision tasks as part of a course project. The report received special mention from course supervisor. `external_link`.","tags":["common-sense","Vision and Language"],"title":"Towards Learning Spatial Common Sense Through Weak Supervision : A Comparitive Study","type":"project"},{"authors":null,"categories":null,"content":"","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"d3aa665bf9c1bcd40bb9aabb0b6aac1b","permalink":"https://ayushjain1144.github.io/project/abb/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/project/abb/","section":"project","summary":"Built simulation of various tasks like writing a cursive and block alphabets on a paper and building complex curved designs using AutoPath, which is finally executed on a real ABB Robot.","tags":["Robotics","ABB"],"title":"ABB Robot Simulation","type":"project"},{"authors":["**Ayush Jain***","N.M. Meenachi","B. Venkatraman"],"categories":null,"content":"Significant advances have been made in recent years on Natural Language Processing with machines surpassing human performance in many tasks, including but not limited to Question Answering. The majority of deep learning methods for Question Answering targets domains with large datasets and highly matured literature. The area of Nuclear and Atomic energy has largely remained unexplored in exploiting available unannotated data for driving industry viable applications. To tackle this issue of lack of quality dataset, this paper intriduces two datasets: NText, a eight million words dataset extracted and preprocessed from nuclear research papers and thesis; and NQuAD, a Nuclear Question Answering Dataset, which contains 700+ nuclear Question Answer pairs developed and verified by expert nuclear researchers. This paper further propose a data efficient technique based on BERT, which improves performance significantly as compared to original BERT baseline on above datasets. Both the datasets, code and pretrained weights will be made publically available, which would hopefully attract more research attraction towards the nuclear domain.\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"1fbabdbc2910a07942a3809a0778d89c","permalink":"https://ayushjain1144.github.io/publication/nukebert/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/nukebert/","section":"publication","summary":"Significant advances have been made in recent years on Natural Language Processing with machines surpassing human performance in many tasks, including but not limited to Question Answering. The majority of deep learning methods for Question Answering targets domains with large datasets and highly matured literature. The area of Nuclear and Atomic energy has largely remained unexplored in exploiting available unannotated data for driving industry viable applications. To tackle this issue of lack of quality dataset, this paper intriduces two datasets: NText, a eight million words dataset extracted and preprocessed from nuclear research papers and thesis; and NQuAD, a Nuclear Question Answering Dataset, which contains 700+ nuclear Question Answer pairs developed and verified by expert nuclear researchers.","tags":null,"title":"NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain","type":"publication"},{"authors":null,"categories":null,"content":"","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"75a0330cb167fa07184a9f21fe0c68c3","permalink":"https://ayushjain1144.github.io/project/ner/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/ner/","section":"project","summary":"Built multilayer perceptron network for Named Entity Recognition on CONLL2003 dataset from scratch, implementing backpropagation and gradient checking. Implemented Synthetic Minority Oversampling Technique to mitigate class imbalance issues, achieving 6 point gain in F1 scores. `external_link`.","tags":["NLP","Named Entity Recognition"],"title":"Named Entity Recognition from scratch","type":"project"},{"authors":null,"categories":null,"content":"","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"d504a20bfbad642106bfc6f7877cf2a3","permalink":"https://ayushjain1144.github.io/project/compiler/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/project/compiler/","section":"project","summary":"erplag-cc is a compiler for the custom language ERPLAG, written in C. The compiler generates assembly code that can be run on a Linux machine.  `external_link`.","tags":["compiler","C"],"title":"Compiler Construction","type":"project"},{"authors":null,"categories":null,"content":"","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"d8e38992bbb57d521ec7eeae6c2fc02a","permalink":"https://ayushjain1144.github.io/project/minesweeper/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/project/minesweeper/","section":"project","summary":"An artificially agent created to play Minesweepers. `external_link`.","tags":["AI","python"],"title":"AI Minesweeper","type":"project"},{"authors":null,"categories":null,"content":"","date":1534464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534464000,"objectID":"df188a091a8e87132c20d648ad1ed4cb","permalink":"https://ayushjain1144.github.io/project/earthquake/","publishdate":"2018-08-17T00:00:00Z","relpermalink":"/project/earthquake/","section":"project","summary":"Implemented Rundle et al.'s research paper for probabilistic prediction of earthquake based on nowcasting. Using the previous nowcast points, a CDF is formed, using which probability of next big earthquake is predicted. This project was made under the guidance of Dr. Sumanta Pasari and received award from Microsoft `external_link`.","tags":["LSTM","Disaster Management"],"title":"Earthquake Prediction and Management","type":"project"}]