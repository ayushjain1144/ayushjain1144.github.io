[
    {"authors":["Nikolaos Gkanatsios*", "**Ayush Jain***", "Zhou Xian", "Yunchu Zhang", "Christopher A. Atketson", "Katerina Fragkiadaki (* Equal Contribution)"],"categories":null,"content":"We propose a model that maps spatial rearrangement instructions to goal scene configurations via gradient descent on a set of relational energy functions over object 2D overhead locations, one per spatial predicate in the instruction. Energy based models over object locations are trained from a handful of examples of object arrangements annotated with the corresponding spatial predicates. Predicates can be binary (e.g., left of, right of, etc.) or multi-ary (e.g., circles, lines, etc.). A language parser maps language instructions to the corresponding set of EBMs, and a visual-language model grounds their arguments on relevant objects in the visual scene. Energy minimization on the joint set of energies iteratively updates the object locations till their final configuration. Then, low-level local policies re-locate objects to the inferred goal locations. Our framework shows many forms of strong generalization: (i)joint energy minimization handles zero-shot complex predicate compositions while each EBM is trained only from single predicate instructions, (ii) the model can execute instructions zero-shot, without a need for paired instruction-action training, (iii) instructions can mention novel objects and attributes at test time thanks to the pre-training of the visual language grounding model from large scale passive captioned datasets. We test the model in established instruction-guided manipulation benchmarks, as well as a benchmark of compositional instructions we introduce in this work. We show large improvements over state-of-the-art end-to-end language to action policies and planning in large language models, especially for long instructions and multi-ary spatial concepts.\n","date":1704880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704880000,"objectID":"353d41e9211e4a4443f0764ecbc735f1","permalink":"http://localhost:1313/publication/robot_coaching/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/publication/robot_coaching/","section":"publication","summary":"We propose a model that maps spatial rearrangement instructions to goal scene configurations via gradient descent on a set of relational energy functions over object 2D overhead locations, one per spatial predicate in the instruction.","tags":null,"title":"Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement","type":"publication"},
    
    {"authors":["**Ayush Jain***","Nikolaos Gkanatsios*","Ishita Mediratta","Katerina Fragkiadaki (* Equal Contribution)"],"categories":null,"content":"Most models tasked to ground referential utterances in 2D and 3D scenes learn to select the referred object from a pool of object proposals provided by a pre-trained detector. This is limiting because an utterance may refer to visual entities at various levels of granularity, such as the chair, the leg of the chair, or the tip of the front leg of the chair, which may be missed by the detector. We propose a language grounding model that attends on the referential utterance and on the object proposal pool computed from a pre-trained detector to decode referenced objects with a detection head, without selecting them from the pool. In this way, it is helped by powerful pre-trained object detectors without being restricted by their misses. We call our model Bottom Up Top Down DEtection TRansformers (BUTD-DETR) because it uses both language guidance (top down) and objectness guidance (bottom-up) to ground referential utterances in images and point clouds. Moreover, BUTD-DETR casts object detection as referential grounding and uses object labels as language prompts to be grounded in the visual scene, augmenting supervision for the referential grounding task in this way. The proposed model sets a new state-of-the-art across popular 3D language grounding benchmarks with significant performance gains over previous 3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When applied in 2D images, it performs on par with the previous state of the art. We ablate the design choices of our model and quantify their contribution to performance. Our code and checkpoints can be found at the project website: https://butd-detr.github.io/\n","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604880000,"objectID":"353d41e9211e4a4443f0764ecbc735f0","permalink":"http://localhost:1313/publication/beauty-detr/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/publication/beauty-detr/","section":"publication","summary":"Most models tasked to ground referential utterances in 2D and 3D scenes learn to select the referred object from a pool of object proposals provided by a pre-trained detector. This is limiting because an utterance may refer to visual entities at various levels of granularity, such as the chair, the leg of the chair, or the tip of the front leg of the chair, which may be missed by the detector.","tags":null,"title":"Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds","type":"publication"},
{"authors":["Zhaoyuan Fang*","**Ayush Jain***","Gabriel Sarch*","Adam Harley","Katerina Fragkiadaki (* Equal Contribution)"],"categories":null,"content":"Humans learn to better understand the world by moving around their environment to get more informative viewpoints of the scene. Most methods for 2D visual recognition tasks such as object detection and segmentation treat images of the same scene as individual samples and do not exploit object permanence in multiple views. Generalization to novel scenes and views thus requires additional training with lots of human annotations. In this paper, we propose a self-supervised framework to improve an object detector in unseen scenarios by moving an agent around in a 3D environment and aggregating multi-view RGB-D information. We unproject confident 2D object detections from the pre-trained detector and perform unsupervised 3D segmentation on the point cloud. The segmented 3D objects are then re-projected to all other views to obtain pseudo-labels for fine-tuning. Experiments on both indoor and outdoor datasets show that (1) our framework performs high quality 3D segmentation from raw RGB-D data and a pre-trained 2D detector; (2) fine-tuning with self supervision improves the 2D detector significantly where an unseen RGB image is given as input at test time; (3) training a 3D detector with self supervision outperforms a comparable self-supervised method by a large margin.\n","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604880000,"objectID":"222ba68cd34653b4e16537df57d98744","permalink":"http://localhost:1313/publication/seeingbymoving/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/publication/seeingbymoving/","section":"publication","summary":"Humans learn to better understand the world by moving around their environment to get more informative viewpoints of the scene. Most methods for 2D visual recognition tasks such as object detection and segmentation treat images of the same scene as individual samples and do not exploit object permanence in multiple views. Generalization to novel scenes and views thus requires additional training with lots of human annotations. In this paper, we propose a self-supervised framework to improve an object detector in unseen scenarios by moving an agent around in a 3D environment and aggregating multi-view RGB-D information.","tags":null,"title":"Move to See Better: Towards Self-Improving Embodied Object Detection","type":"publication"},{"authors":["**Ayush Jain***","Rohit Ramaprasad*","Pratik Narang","Murari Mandal","Vinay Chamola","F. Richard Yu","Mohsen Guizani (* Equal Contribution)"],"categories":null,"content":"Unmanned Aerial Vehicles (UAVs) are emerging as a powerful tool for various industrial and smart city applications. The UAVs coupled with various sensors can perform many cognitive tasks such as object detection, surveillance, traffic management, and urban planning. These tasks often rely on computationally expensive deep learning approaches. Execution of the compute intensive algorithms are usually not feasible with the embedded processors on a power-constrained UAV. Therefore, the Edge-AI has emerged as a popular alternative in such scenarios by offloading the heavy-lifting tasks to the Edge devices. This work proposes a deep learning approach for detection of objects in aerial scenes captured by UAVs. In our setup, the power-constrained drone is used only for data collection, while the computationally intensive tasks are offloaded to a GPU edge server. Our work first categorize the current methods for aerial object detection using deep learning techniques and discusses how the task is different from general object detection scenarios. We delineate the specific challenges involved and experimentally demonstrate the key design decisions which significantly affect the accuracy and robustness of model. We further propose an optimized architecture which utilizes these optimal design choices along with the recent ResNeSt backbone in order to achieve superior performance in aerial object detection. Lastly, we propose several research directions to inspire further advancement in aerial object detection.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"b21fa14ddc93d5eab2848d121bf265af","permalink":"http://localhost:1313/publication/aerial_object_detection/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/aerial_object_detection/","section":"publication","summary":"Unmanned Aerial Vehicles (UAVs) are emerging as a powerful tool for various industrial and smart city applications. The UAVs coupled with various sensors can perform many cognitive tasks such as object detection, surveillance, traffic management, and urban planning. These tasks often rely on computationally expensive deep learning approaches. Execution of the compute intensive algorithms are usually not feasible with the embedded processors on a power-constrained UAV. Therefore, the Edge-AI has emerged as a popular alternative in such scenarios by offloading the heavy-lifting tasks to the Edge devices.","tags":null,"title":"AI-enabled Object Detection in Unmanned Aerial Vehicles for Edge Computing Applications","type":"publication"},{"authors":["Dawei Du","Longyin Wen","Pengfei Zhu","Heng Fan","Qinghua Hu","Haibin Ling","Mubarak Shah","Junwen Pan","**Ayush Jain**","Pratik Narang","et al."],"categories":null,"content":"The Vision Meets Drone Object Detection in Image Challenge (VisDrone-DET 2020) is the third annual object detector benchmarking activity. Compared with the previous VisDrone-DET 2018 and VisDrone-DET 2019 challenges, many submitted object detectors exceed the recent state-of-the-art detectors. Based on the selected 29 robust detection methods, we discuss the experimental results comprehensively, which shows the effectiveness of ensemble learning and data augmentation in drone captured object detection. The full challenge results are publicly available at the website http://aiskyeye.com/leaderboard/.\n","date":1595980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595980800,"objectID":"d184cec039e052a88c8cc200c0da9c82","permalink":"http://localhost:1313/publication/visdron/","publishdate":"2020-07-29T00:00:00Z","relpermalink":"/publication/visdron/","section":"publication","summary":"The Vision Meets Drone Object Detection in Image Challenge (VisDrone-DET 2020) is the third annual object detector benchmarking activity. Compared with the previous VisDrone-DET 2018 and VisDrone-DET 2019 challenges, many submitted object detectors exceed the recent state-of-the-art detectors. Based on the selected 29 robust detection methods, we discuss the experimental results comprehensively, which shows the effectiveness of ensemble learning and data augmentation in drone captured object detection. The full challenge results are publicly available at the website http://aiskyeye.","tags":null,"title":"VisDrone-DET2020: The Vision Meets Drone Object Detection in Image Challenge Results","type":"publication"},{"authors":null,"categories":null,"content":"","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"1a032ba59ab990085b0223efdbe424ae","permalink":"http://localhost:1313/project/grnn/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/project/grnn/","section":"project","summary":"Reviewed, compared and analysed three recent papers proposed in generalising to novel views through the use of view-prediction self-supervision tasks as part of a course project. The report received special mention from course supervisor. `external_link`.","tags":["common-sense","Vision and Language"],"title":"Towards Learning Spatial Common Sense Through Weak Supervision : A Comparitive Study","type":"project"},{"authors":null,"categories":null,"content":"","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"d3aa665bf9c1bcd40bb9aabb0b6aac1b","permalink":"http://localhost:1313/project/abb/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/project/abb/","section":"project","summary":"Built simulation of various tasks like writing a cursive and block alphabets on a paper and building complex curved designs using AutoPath, which is finally executed on a real ABB Robot.","tags":["Robotics","ABB"],"title":"ABB Robot Simulation","type":"project"},{"authors":["**Ayush Jain***","N.M. Meenachi","B. Venkatraman"],"categories":null,"content":"Significant advances have been made in recent years on Natural Language Processing with machines surpassing human performance in many tasks, including but not limited to Question Answering. The majority of deep learning methods for Question Answering targets domains with large datasets and highly matured literature. The area of Nuclear and Atomic energy has largely remained unexplored in exploiting available unannotated data for driving industry viable applications. To tackle this issue of lack of quality dataset, this paper intriduces two datasets: NText, a eight million words dataset extracted and preprocessed from nuclear research papers and thesis; and NQuAD, a Nuclear Question Answering Dataset, which contains 700+ nuclear Question Answer pairs developed and verified by expert nuclear researchers. This paper further propose a data efficient technique based on BERT, which improves performance significantly as compared to original BERT baseline on above datasets. Both the datasets, code and pretrained weights will be made publically available, which would hopefully attract more research attraction towards the nuclear domain.\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"1fbabdbc2910a07942a3809a0778d89c","permalink":"http://localhost:1313/publication/nukebert/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/nukebert/","section":"publication","summary":"Significant advances have been made in recent years on Natural Language Processing with machines surpassing human performance in many tasks, including but not limited to Question Answering. The majority of deep learning methods for Question Answering targets domains with large datasets and highly matured literature. The area of Nuclear and Atomic energy has largely remained unexplored in exploiting available unannotated data for driving industry viable applications. To tackle this issue of lack of quality dataset, this paper intriduces two datasets: NText, a eight million words dataset extracted and preprocessed from nuclear research papers and thesis; and NQuAD, a Nuclear Question Answering Dataset, which contains 700+ nuclear Question Answer pairs developed and verified by expert nuclear researchers.","tags":null,"title":"NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain","type":"publication"},{"authors":null,"categories":null,"content":"","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"75a0330cb167fa07184a9f21fe0c68c3","permalink":"http://localhost:1313/project/ner/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/ner/","section":"project","summary":"Built multilayer perceptron network for Named Entity Recognition on CONLL2003 dataset from scratch, implementing backpropagation and gradient checking. Implemented Synthetic Minority Oversampling Technique to mitigate class imbalance issues, achieving 6 point gain in F1 scores. `external_link`.","tags":["NLP","Named Entity Recognition"],"title":"Named Entity Recognition from scratch","type":"project"},{"authors":null,"categories":null,"content":"","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"d504a20bfbad642106bfc6f7877cf2a3","permalink":"http://localhost:1313/project/compiler/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/project/compiler/","section":"project","summary":"erplag-cc is a compiler for the custom language ERPLAG, written in C. The compiler generates assembly code that can be run on a Linux machine.  `external_link`.","tags":["compiler","C"],"title":"Compiler Construction","type":"project"},{"authors":null,"categories":null,"content":"","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"d8e38992bbb57d521ec7eeae6c2fc02a","permalink":"http://localhost:1313/project/minesweeper/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/project/minesweeper/","section":"project","summary":"An artificially agent created to play Minesweepers. `external_link`.","tags":["AI","python"],"title":"AI Minesweeper","type":"project"},{"authors":null,"categories":null,"content":"","date":1534464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534464000,"objectID":"df188a091a8e87132c20d648ad1ed4cb","permalink":"http://localhost:1313/project/earthquake/","publishdate":"2018-08-17T00:00:00Z","relpermalink":"/project/earthquake/","section":"project","summary":"Implemented Rundle et al.'s research paper for probabilistic prediction of earthquake based on nowcasting. Using the previous nowcast points, a CDF is formed, using which probability of next big earthquake is predicted. This project was made under the guidance of Dr. Sumanta Pasari and received award from Microsoft `external_link`.","tags":["LSTM","Disaster Management"],"title":"Earthquake Prediction and Management","type":"project"}]
